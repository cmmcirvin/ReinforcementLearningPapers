{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Monte Carlo ES (Sutton and Barto, section 5.3, page 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OpenAI's gym, blackjack has an observation space consisting of a 3-tuple of\n",
    "\n",
    "The player's current sum\n",
    "\n",
    "The value of dealer's one showing card (1 - 10)\n",
    "\n",
    "Whether or not the player has an ace (0 or 1) - important because aces can be either 1 or 11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space consists of two values, stick/stop (0) and hit / draw a card (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, gamma=0.85):\n",
    "        self.env = gym.make(\"Blackjack-v1\", natural=False) # Blackjack environment from the OpenAI gym\n",
    "        self.sum_size = self.env.observation_space[0].n # Number of possibilities for the player's current sum\n",
    "        self.dealer_size = self.env.observation_space[1].n # Number of possibilities for the dealer's face up card\n",
    "        self.gamma = gamma\n",
    "        self.policy = self.initializeBlackjackPolicy()\n",
    "        self.Q = self.initializeQ()\n",
    "        self.returns = self.initializeReturns()\n",
    "\n",
    "    '''\n",
    "    Create an empty returns list\n",
    "    '''\n",
    "    def initializeReturns(self):\n",
    "\n",
    "        returns = {}\n",
    "\n",
    "        for sum in range(self.sum_size):\n",
    "            for dealer_value in range(self.dealer_size):\n",
    "                for action in range(self.env.action_space.n):\n",
    "                    returns[((sum, dealer_value), action)] = (0, 0) # Initialize returns to a tuple of the form (number_of_samples, average_reward)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    '''\n",
    "    Initializes each state's policy to a random initial value\n",
    "    '''\n",
    "    def initializeBlackjackPolicy(self):\n",
    "        policy = {}\n",
    "\n",
    "        for sum in range(self.sum_size):\n",
    "            for dealer_value in range(self.dealer_size):\n",
    "                policy[(sum, dealer_value)] = 0 # Initialize policy arbitrarily\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    '''\n",
    "    Creates a Q function of zeros for each state action pair\n",
    "    '''\n",
    "    def initializeQ(self):\n",
    "        Q = {}\n",
    "\n",
    "        for sum in range(self.sum_size):\n",
    "            for dealer_value in range(self.dealer_size):\n",
    "                for action in range(self.env.action_space.n):\n",
    "                    Q[((sum, dealer_value), action)] = 0 # Initialize Q arbitrarily\n",
    "        \n",
    "        return Q\n",
    "    \n",
    "    '''\n",
    "    Returns a random action to take and gain experience from\n",
    "    Primarily for testing purposes, inefficient in practice\n",
    "    '''\n",
    "    def getRandomAction(self, observation):\n",
    "        return np.random.randint(0, self.env.action_space.n)\n",
    "\n",
    "    '''\n",
    "    Follows a policy to select an action\n",
    "    '''\n",
    "    def getActionFromPolicy(self, observation):\n",
    "        S_t_dim_0 = observation[0]\n",
    "        S_t_dim_1 = observation[1]\n",
    "\n",
    "        if np.random.randint(0, 10) >= 7: # Adding in some randomness to avoid potential initial pitfalls\n",
    "            return self.getRandomAction(observation)\n",
    "        \n",
    "        return self.policy[(S_t_dim_0, S_t_dim_1)]\n",
    "    \n",
    "    '''\n",
    "    Generates an action based on the policy to follow\n",
    "    '''\n",
    "    def generateEpisode(self, actionType=getActionFromPolicy):\n",
    "        observation = self.env.reset()\n",
    "        observation = (observation[0][0], observation[0][1])\n",
    "        episode = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = actionType(self, observation)\n",
    "            new_observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "            new_state = (observation[0], observation[1])\n",
    "            episode.append((new_state, action, reward))\n",
    "            observation = new_observation\n",
    "\n",
    "            done = terminated or truncated\n",
    "        return episode\n",
    "\n",
    "    def updateAfterEpisode(self, episode):\n",
    "        G = 0\n",
    "        for step in reversed(range(len(episode))):\n",
    "            state = (episode[step][0][0], episode[step][0][1])\n",
    "            \n",
    "            A_t = episode[step][1]\n",
    "\n",
    "            reward = episode[step][2]\n",
    "            G = self.gamma * G + reward\n",
    "\n",
    "            if episode[step][0] in episode[:step]: # Implement first look only\n",
    "                print(\"not first\")\n",
    "                continue\n",
    "\n",
    "            self.returns[(state, A_t)] = (self.returns[(state, A_t)][0] + 1, (self.returns[(state, A_t)][0] * self.returns[(state, A_t)][1] + G) / (self.returns[(state, A_t)][0] + 1))\n",
    "            self.Q[(state, A_t)] = self.returns[(state, A_t)][1]\n",
    "            self.policy[state] = np.argmax([self.Q[x] for x in self.Q if x[0] == state])\n",
    "\n",
    "    def monteCarloES(self, gamma=0.85, num_episodes=200000):\n",
    "        for ep in tqdm(range(num_episodes)):\n",
    "            observation, info = self.env.reset()\n",
    "            episode = self.generateEpisode()\n",
    "            self.updateAfterEpisode(episode)\n",
    "    \n",
    "    def displayPolicy(self):\n",
    "        print(\"   \", end=\"\")\n",
    "        for dealer in range(1, 11):\n",
    "            print(\" %03d \" % dealer, end=\"\")\n",
    "        print()\n",
    "        for sum in range(11, 21):\n",
    "            print(\"%d \" % sum, end=\"\")\n",
    "            for dealer in range(1, 11):\n",
    "                print(\"  %d  \" % self.policy[(sum, dealer)], end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:55<00:00, 3610.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    001  002  003  004  005  006  007  008  009  010 \n",
      "11   1    1    1    1    1    1    1    1    1    1  \n",
      "12   1    0    0    0    0    0    1    1    1    1  \n",
      "13   1    0    0    0    0    0    1    1    1    1  \n",
      "14   1    0    0    0    0    0    1    1    1    1  \n",
      "15   1    0    0    0    0    0    1    1    1    1  \n",
      "16   1    0    0    0    0    0    1    1    0    0  \n",
      "17   1    0    0    0    0    0    0    0    0    0  \n",
      "18   0    0    0    0    0    0    0    0    0    0  \n",
      "19   0    0    0    0    0    0    0    0    0    0  \n",
      "20   0    0    0    0    0    0    0    0    0    0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = Agent()\n",
    "a.monteCarloES()\n",
    "a.displayPolicy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual testing to play around with the mechanics of the gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7, False)\n",
      "(7, 7, False) 0 -1.0\n",
      "(7, 9, False)\n",
      "(7, 9, False) 0 1.0\n",
      "(9, 10, False)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m terminated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminated:\n\u001b[1;32m---> 10\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m())\n\u001b[0;32m     11\u001b[0m     observation, reward, terminated, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(observation, action, reward)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Blackjack-v1\", natural=False)\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    if observation[0] > observation[1]:\n",
    "        continue\n",
    "    \n",
    "    print(observation)\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = int(input())\n",
    "        observation, reward, terminated, _, _ = env.step(action)\n",
    "        print(observation, action, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcd4837add793386cd67e25a441830de5b4a5ceabdd5dcba5a317ceae751db5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
